{
  "slug": "measuring-productivity-in-the-age-of-ai",
  "title": "Measuring Productivity in the Age of AI",
  "description": "In a world where AI can create all the code, your job title is changing from \"engineer\" to \"engineer of engineers\" what is the new scoreboard?",
  "date": "2026-02-22T00:00:00.000Z",
  "coverImageUrl": "https://raw.githubusercontent.com/sbpete/blog/main/images/measuring-productivity-in-the-age-of-ai-1771797461005-gemini_generated_image_szdxs4szdxs4szdx.png",
  "tags": [
    "AI",
    "Productivity",
    "Software Engineering"
  ],
  "contentMarkdown": "The cost of producing good code has been falling fast. Since the rise of capable AI models, even seasoned, high-caliber engineers are reporting meaningful productivity speedups, and the reason isn't hard to see. Software engineering is a near-perfect environment for LLM improvement: inputs and outputs are text-based, correctness is often deterministic, and the internet is saturated with training examples. As a result, LLM performance has caught up with top software engineers faster than in almost any other field.\n\n![Generated_chart__chart.png.png](https://raw.githubusercontent.com/sbpete/blog/main/images/measuring-productivity-in-the-age-of-ai-1771796780175-generated_chart__chart.png.png)\n\nStudies back this up, though the numbers vary widely by task type. The original GitHub Copilot study found developers completed a simple HTTP server task **55.8% faster** with AI assistance. Enterprise settings are more modest. A multi-company study found roughly a **26% average boost** in completed tasks, with junior developers seeing gains of 27-39% and senior developers seeing only 8-13%. Meanwhile, the 2025 DORA/Faros \"AI Productivity Paradox\" report, analyzing over 10,000 developers, found that high-AI teams merged 98% more pull requests, but PR review times ballooned by 91%, meaning the extra code simply queued up waiting for humans. The speedup is real, but it's uneven and context-dependent.\n\n## The Wrong Yardstick\n\nSo what *should* we be measuring? There's a lot of noise around how much code these models produce, but lines of code has never been a reliable indicator of good engineering, and it's even less meaningful now. Models are optimized for correctness and speed, not elegance, which means they can produce verbose, brittle solutions that pass tests but create maintenance headaches down the road.\n\nBenchmarks have the same problem. They're useful for comparing models against each other, but they suffer from the same flaw as standardized tests for students: performance on a curated test doesn't reliably generalize to the messy, ambiguous problems that show up in real corporate codebases. A model that aces HumanEval may still fumble when asked to untangle a legacy monolith with undocumented business logic.\n\n## Business Value Is the Real Signal\n\nThe most honest way to evaluate an AI coding tool is to measure its actual business impact, and counterintuitively, this is *easier* to do for AI than for human workers. Code shipping velocity, incident rates, time-to-feature, and bug density are all trackable. The catch is that your organization has to be set up to capture them.\n\nBefore letting agents run loose in a codebase, give them defined boundaries. A backend agent and a frontend agent that share awareness of the same endpoints but are each solely responsible for their own layer is a practical starting point. One agent can call utility functions managed entirely by another. This kind of separation does double duty: it keeps context windows small (which directly improves model performance) and makes attribution cleaner when you're trying to measure outcomes. Organizations that build tight monitoring and feedback loops around their AI usage will compound their gains over time; those that don't will plateau.\n\n## The Harder Problem: Measuring Human Productivity\n\nMeasuring AI output is actually the easier half of this problem. The harder part is figuring out what \"good\" looks like for the engineers orchestrating these systems. What criteria define someone who is genuinely skilled at working with AI? Prompting fluency? The ability to provide rich context for complex, proprietary codebases? Those skills matter, but they also have a short shelf life. A certification in AI loses its value far faster than one in AWS or GCP, because many of the techniques that were cutting-edge eighteen months ago are already obsolete.\n\nThe old playbook, coding bootcamp or CS degree followed by a job offer on graduation day, no longer holds by itself. In a world where the code is increasingly handled for you, the only durable measure is the business value you drive. You need to be able to point to concrete outcomes and say: *I shipped this, it unblocked that, it saved X hours or generated Y revenue.* The abstraction layer is rising, and the engineers who thrive will be the ones who move up with it rather than defending the layer below.\n\n## Making Your Impact Legible\n\nA few habits that help you think and talk in terms of business value:\n\n- **Write down what you worked on**, not just task titles but the downstream effect. \"Fixed auth bug\" is forgettable; \"fixed auth bug that was blocking three enterprise onboarding calls\" is not.\n- **Prioritize hard, messy problems over tedious tasks.** AI handles the tedious work well; your value is in the judgment calls it can't make.\n- **Volunteer for new, innovative projects.** With the current pace of tooling change, there is no shortage of greenfield work that requires human taste and architectural thinking.\n- **Track your own feedback loops.** Just as organizations should monitor AI tool performance, you should monitor your own. What kinds of problems do you solve faster with AI? Where does it slow you down? That self-knowledge is genuinely rare.\n"
}
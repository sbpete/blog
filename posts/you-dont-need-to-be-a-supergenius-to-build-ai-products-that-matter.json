{
  "slug": "you-dont-need-to-be-a-supergenius-to-build-ai-products-that-matter",
  "title": "# You Don’t Need to Be a Supergenius to Build AI Products That Matter",
  "description": "Most people won’t train models, but the real leverage in AI comes from knowing when to use it, how to shape it, and how to keep it useful instead of noisy.",
  "date": "2026-01-11T00:00:00.000Z",
  "coverImageUrl": "https://raw.githubusercontent.com/sbpete/blog/main/images/you-dont-need-to-be-a-supergenius-to-build-ai-products-that-matter-1768170186622-gemini_generated_image_660was660was660w.png",
  "tags": [
    "AI",
    "Product Design",
    "Product Strategy"
  ],
  "contentMarkdown": "Not everyone is a supergenius who can land a researcher role at a frontier model provider. Most people will never train a foundation model, publish benchmark-topping papers, or work on core architecture at places like OpenAI, Anthropic, or DeepMind.\n\nThat is fine.\n\nAI skills are still in rapidly growing demand because most of the real leverage comes *after* the model exists. AI product development, infrastructure and platforms, MLOps, and system security are where models turn into real value. This post focuses on AI product thinking and philosophy, not AI engineering or systems design.\n\nBuilding generative applications requires a practical understanding of model capabilities, how they behave under ambiguity, and how to consistently extract useful output. Just as importantly, it requires product judgment. Without that, AI products tend to regress into slop that looks impressive but delivers little value.\n\nBefore adding AI to anything, a few questions should be non-negotiable:\n\n- Is this product actually better with AI?\n- Who is this saving time or money for?\n- Is this product something that is possible without AI?\n\nThat last question matters more than it sounds. If the product works just as well without AI, then AI is likely a liability, not an advantage.\n\n---\n\n## Generative AI Amplifies Weak Product Thinking\n\nGenerative AI does not magically fix bad products. It accelerates them.\n\nIf the underlying product goal is unclear, AI will generate plausible-sounding output that masks the problem. If success criteria are vague, AI will optimize for sounding helpful rather than being correct. This is why so many AI demos feel impressive for five minutes and useless after a week.\n\nEvidence increasingly supports this. Productivity gains from AI tend to appear only when tasks are well scoped and feedback loops exist. Otherwise, output quality becomes noisy and unreliable.\n\nAnother common failure mode is overcorrecting. Teams respond to hallucinations by adding rigid prompts, excessive guardrails, and brittle instructions. This often backfires. As models improve, these constraints can reduce performance, collapse creativity, and introduce subtle failure cases that are hard to diagnose.\n\n---\n\n## Model Agnosticism Is a Product Strategy\n\nOne core principle of strong generative AI products is model agnosticism.\n\nModels are the worst they will ever be. Hallucination rates are dropping. Reasoning is improving. Instruction following is getting better. Systems that hardcode assumptions about a single model age poorly.\n\nUnless compliance or security constraints demand otherwise, product systems should impose as few hard restrictions as possible. Instead, focus on:\n- Supplying better context\n- Providing concrete examples\n- Encouraging iteration and experimentation\n\nEven instructions can have unintended consequences when they become too tight or overly prescriptive. Small changes in wording can introduce conflicting constraints inside the model.\n\nThe pace of improvement across models is visible in benchmark data. The chart below shows steady gains in GPQA performance across multiple leading models over time, reinforcing the idea that today’s limitations should not be treated as fixed constraints.\n\n![Model improvement over time according to GPQA benchmark](https://media.licdn.com/dms/image/v2/D4E22AQFmkIuuPCQrTw/feedshare-shrink_2048_1536/B4EZsuC6oJIoA0-/0/1766004075588?e=1769644800&v=beta&t=fIwiIbfFU3DKrtWBblMdqcxUNVsslwcH0-IxfybkCcg)\n\nThis is why flexibility matters. Systems that rely on better context and intent understanding tend to automatically improve as models improve underneath them.\n\n---\n\n## Human in the Loop Is Not Optional\n\nHigh quality generative products do not come from longer prompts or more toggles. They come from shared understanding.\n\nOne approach I am currently working on is an AI prompt refinement extension that removes cluttered presets like expert mode or add examples. Nothing is pre-selected. Instead, the system focuses on fulfilling core prompt principles such as context, specificity, intent, and response format based on what the user is actually trying to do.\n\nThe system uses human-in-the-loop questioning. If intent is unclear, it asks follow-up questions instead of guessing. This shifts the goal from producing pleasing output to producing correct output.\n\nAllowing users to bring their own API keys reinforces this philosophy. It avoids vendor lock-in, keeps users in control, and preserves model agnosticism as models evolve.\n\n---\n\n## Closing Thoughts\n\nYou do not need to be an AI researcher to work on meaningful AI problems. The biggest gaps right now are not in model architecture. They are in judgment.\n\nKnowing when to use AI and when not to. Knowing how to guide models without suffocating them. Knowing how to design systems that improve as models improve.\n\nThe people who get the most leverage from AI will not be the ones chasing every new model release. They will be the ones who understand how humans and models actually work together, and who build products that respect both.\n\n---\n\n## References and Further Reading\n\n- Ethan Mollick, *Using AI to Actually Make Things Better* (Harvard Business Review)  \n  https://hbr.org/2023/04/how-to-use-ai-to-actually-make-things-better\n\n- McKinsey Global Institute, *The Economic Potential of Generative AI*  \n  https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai\n\n- Stanford Human-Centered AI, *Foundation Models in Practice*  \n  https://hai.stanford.edu/research/foundation-models\n\n- OpenAI, *Prompt Engineering Best Practices*  \n  https://platform.openai.com/docs/guides/prompt-engineering\n\n- Anthropic, *Constitutional AI: Harmlessness from AI Feedback*  \n  https://www.anthropic.com/research/constitutional-ai\n\n- Microsoft Research, *Prompting vs Fine-Tuning in Production Systems*  \n  https://www.microsoft.com/en-us/research/publication/prompting-or-fine-tuning/\n\n- GPQA Benchmark Overview  \n  https://github.com/idavidrein/gpqa\n"
}
{
  "slug": "dangers-of-ai-in-the-workplace",
  "title": "Dangers of AI in the Workplace",
  "description": "Almost everyone agrees that AI should increase productivity, but how could it go wrong?",
  "date": "2025-11-04T00:00:00.000Z",
  "coverImageUrl": "https://raw.githubusercontent.com/sbpete/blog/main/images/temp-1762293833356-boards.png",
  "tags": [
    "AI",
    "Software Engineering",
    "Product Management",
    "Project Management"
  ],
  "contentMarkdown": "# The Dangers of AI in the Workplace\n\n*As always, a disclaimer before we start — this is purely subjective. Whether you're a student exploring AI tools or a professional already using them daily, I hope these thoughts help you think more critically about what’s happening in the industry.*\n\nAI has become the shiny new tool that promises to speed up everything — from writing code to generating business reports. But like most technologies, its impact depends less on the tool itself and more on how people use it. And people vary **vastly** in how they perceive and interact with AI.\n\nA recent study from the ACM found that engineers’ productivity gains from large language models (LLMs) are often tied directly to their *trust*, *experience*, and *understanding* of the tool’s capabilities ([source](https://dl.acm.org/doi/pdf/10.1145/3643795.3648379)). In other words — the more you know about what AI *can’t* do, the better you can use it.\n\n---\n\n## The New Stress of “Speed”\n\nSoftware development has always been an exercise in estimation — how long will it take to ship a feature, fix a bug, or deliver a release? But with AI promising faster development every few weeks, discrepancies between engineers, product managers, and executives are bound to grow.\n\nNew frameworks, assistants, and “10x productivity” tools drop almost daily. Each one claims to revolutionize your stack. Managers feel pressure to pick the “right” one. Engineers feel pressure to ship faster. Executives expect the output graphs to keep trending upward.\n\nAnd in that rush, testing often becomes an afterthought. Teams skip code reviews. Engineers ship half-baked features to hit deadlines. Codebases balloon into unmanageable messes. The irony? AI was supposed to *reduce* chaos — not amplify it.\n\nBut this is where the misalignment begins. Product managers are told AI will let developers deliver in half the time, while developers know that integrating new AI tools often slows you down before it speeds you up. The “AI productivity boost” that executives expect can create unrealistic timelines that ripple across the organization — causing stress, burnout, and resentment.  \n\nAnd this pressure trickles down: developers cut corners, testing cycles shrink, and long-term maintainability is sacrificed for short-term delivery. In the end, the very tools meant to help us move faster can create **organizational drag** — not because the tech is bad, but because the expectations around it are.\n\n---\n\n## When Engineers Stop Learning\n\nIf AI becomes a shortcut for every problem, developers risk losing their edge in the fundamentals — the debugging instincts, the architectural reasoning, the ability to think deeply about *why* something broke instead of just asking the model to fix it.\n\nDebugging is one of the most valuable skills in engineering — it’s how you build intuition. You learn how systems interact, where assumptions break down, and why design decisions matter. If every “bug” just becomes an AI prompt away from being fixed, you lose that muscle memory. Over time, teams become over-reliant on AI, but under-equipped to handle the edge cases where it fails.\n\nThe worst part? AI-generated code often *looks right*. It compiles. It runs. It may even pass your tests. But it might be doing something subtly incorrect — mishandling data, misusing an API, or introducing a security flaw. And because it was generated rather than understood, it’s much harder to debug later.  \n\nAs engineers, our value isn’t in knowing syntax — it’s in knowing how to think critically. We need to understand why a system behaves the way it does, how to model problems, and how to find the root cause when something breaks. AI can’t replace that reasoning; it can only hide it behind a shiny layer of convenience.\n\n---\n\n## The Mirage of “Business Value”\n\nAI can also run wild in organizations that chase productivity metrics over impact. You see this everywhere: dashboards full of “AI-generated features” that never actually solve a user’s problem.\n\nIt’s easy to confuse *activity* with *progress*. Teams start building AI integrations because they can, not because they should. The business value gets lost somewhere between the prompt and the PowerPoint presentation.  \n\nSoftware engineers often forget that our job isn’t to write code — it’s to create **business value** through software. If your shiny new AI feature doesn’t make someone’s life easier, save them time, or earn the company money, then it’s not progress. It’s noise.\n\nAI, for all its brilliance, doesn’t understand *why* something is valuable. It doesn’t feel user frustration. It doesn’t see business trade-offs. Only humans can make those judgment calls. This is where domain knowledge, empathy, and business awareness separate great engineers from mediocre ones.\n\nA poorly aimed AI project can drain resources, confuse users, and even erode trust in the product. We’ve already seen this with “AI assistants” that provide useless recommendations or write content that’s grammatically correct but contextually tone-deaf. When AI becomes a checkbox for innovation rather than a means to create value, it becomes a liability.\n\n---\n\n## The Culture Gap: Engineers vs. Managers vs. Executives\n\nOne of the most overlooked dangers of AI in the workplace is how it widens the perception gap between different layers of an organization.\n\nExecutives hear that AI can “write code instantly.” Product managers think this means features can ship twice as fast. Meanwhile, engineers know that even if AI helps write code, it doesn’t understand architecture, testing, or edge cases. It doesn’t handle deployments, security, or infrastructure.  \n\nThis disconnect creates tension. When leadership assumes that engineering timelines should shrink because “we have AI now,” developers are set up to fail. They’re forced to meet impossible expectations while still maintaining quality and stability.\n\nIronically, the solution to this isn’t *more* AI — it’s better communication. Teams need to realign on what AI is actually good at: accelerating well-understood, repetitive tasks — not replacing the hard, ambiguous parts of software development that require human judgment.\n\n---\n\n## How We Fix It\n\nWe don’t fix this by banning AI — we fix it by defining **clear boundaries** between what AI does and what humans do.\n\nEthan Mollick, in his book *[Co-Intelligence](https://www.co-intelligence.com/)*, draws the line between *cyborgs* (where AI and human work are blended beyond recognition) and *centaurs* (where AI assists humans but doesn’t replace them). The centaur model works because it preserves human judgment — the part that actually matters.\n\nWe should treat AI as a powerful *collaborator*, not an *executor*. The human remains in charge of direction, purpose, and validation. The AI helps with execution, speed, and iteration.\n\nTo make that collaboration work, we need to return to the fundamentals of good engineering:\n\n- **Testing**: AI can generate tests, but humans must still define what correctness means and decide which edge cases matter.  \n- **Modularity**: AI will happily write monolithic functions that “just work,” but humans must enforce structure, readability, and long-term maintainability.  \n- **Code Reviews**: AI doesn’t understand business context — only humans can review whether the logic aligns with real-world goals and constraints.  \n- **Documentation**: AI can summarize, but it can’t *understand*. Humans still need to ensure clarity for future developers.  \n\nThese fundamentals have guided software for decades. They are the reason systems survive. If anything, AI makes them more important, because every AI-written line of code is a potential black box — and the only antidote to a black box is clarity.\n\n---\n\n## The Real Role of AI\n\nAI won’t replace entire jobs. It’ll replace *tasks* — the tedious, repetitive, or low-stakes ones. And that’s a good thing. The engineers who thrive in the AI era will be the ones who know *how* to use it as leverage, not as a crutch.\n\nThe best developers won’t be the ones who write code fastest, but the ones who understand how to frame problems so that AI can solve them effectively — and when to step in with human judgment.\n\nSo, no — AI isn’t going to take your job tomorrow. But the people who use it well might.\n\nIf AI really can take away the drudge work, it means we get to spend more time doing what actually matters: building things of value, solving real problems, and thinking deeply about how technology fits into the messy, human world we live in.\n\n---\n\n**Further Reading**\n\n- [“Things They Didn’t Teach You About Software Engineering”](https://vadimkravcenko.com/shorts/things-they-didnt-teach-you/) by Vadim Kravcenko  \n- [ACM Study on LLM Perception and Usefulness](https://dl.acm.org/doi/pdf/10.1145/3643795.3648379)  \n- *Co-Intelligence* by Ethan Mollick  \n\n---\n\n*AI won’t destroy software engineering — but it will expose who truly understands it.*\n"
}